{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./resources/System_v2_custom_partI.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content\n",
    "\n",
    "- [Introduction](###-Introduction)\n",
    "- [How to run this notebook?](###-How-to-run-this-notebook?)\n",
    "- [Before you start](###-Before-you-start)\n",
    "- [1. Get your private reference data](###-1.-Get-your-private-reference-data)\n",
    "- [2. Prepare your reference data](###-2.-Prepare-your-reference-data)\n",
    "- [3. EO data extractions](###-3.-EO-data-extractions)\n",
    "- [4. Inspect results](###-4.-Inspect-results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "The following demo illustrates how to train and deploy your custom crop type model, based on a combination of your own private reference data and publicly available reference data, using the WorldCereal system.<br>\n",
    "\n",
    "The demo has been split into two parts:\n",
    "\n",
    "- **Part 1** (this part) deals with preparing your private data so it can be used for training a crop type model. This involves uploading your data to the [WorldCereal Reference Data Module](https://rdm.esa-worldcereal.org/) and extracting the relevant EO data time series for all your samples to be used for model training. Extractions are done through OpenEO, from the [Copernicus Data Space Ecosystem](https://dataspace.copernicus.eu/) cloud backend. After extracting the EO data, we perform a quality check on the extracted data.\n",
    "\n",
    "- **Part 2** (see separate notebook) proceeds with selecting the right training data for your application, trains the crop type model, deploys it in the cloud and produces a map based on your custom model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to run this notebook?\n",
    "\n",
    "#### Option 1: Run on Terrascope\n",
    "\n",
    "You can use a preconfigured environment on [**Terrascope**](https://terrascope.be/en) to run the workflows in a Jupyter notebook environment. Just register as a new user on Terrascope or use one of the supported EGI eduGAIN login methods to get started.\n",
    "\n",
    "Once you have a Terrascope account, you can run this notebook by clicking the button shown below.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">When you click the button, you will be prompted with \"Server Options\". Make sure to select the \"Worldcereal\" image here. Did you choose \"Terrascope\" by accident? Then go to File > Hub Control Panel > Stop my server, and click the link below once again.</div>\n",
    "\n",
    "<a href=\"https://notebooks.terrascope.be/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2FWorldCereal%2Fworldcereal-classification&urlpath=lab%2Ftree%2Fworldcereal-classification%2Fnotebooks%2Fworldcereal_v2_demo_custom_model_training_PART1.ipynb&branch=main\"><img src=\"https://img.shields.io/badge/Run%20System%20v2%20demo%20Part1-Terrascope-brightgreen\" alt=\"Run System V2 demo Part 1\" valign=\"middle\"></a>\n",
    "\n",
    "\n",
    "#### Option 2: Install Locally\n",
    "\n",
    "If you prefer to install the package locally, you can create the WorldCereal environment using **Conda** or **pip**.\n",
    "\n",
    "First clone the repository:\n",
    "```bash\n",
    "git clone https://github.com/WorldCereal/worldcereal-classification.git\n",
    "cd worldcereal-classification\n",
    "```\n",
    "Next, install the package locally:\n",
    "- for Conda: `conda env create -f environment.yml`\n",
    "- for Pip: `pip install .[train,notebooks]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before you start\n",
    "\n",
    "To be able to use all functionality in this notebook, you will need to register for:\n",
    "- a free [Terrascope](https://terrascope.be/en) account\n",
    "- a free [Copernicus Data Space Ecosystem (CDSE)](https://dataspace.copernicus.eu/) account\n",
    "\n",
    "In addition, you are required to **upload your private reference dataset to the WorldCeral Reference Data Module**, using the highly automated upload workflow in our [user interface](https://rdm.esa-worldcereal.org/). Please consult the following resources to find out how to prepare and upload your dataset:\n",
    "- [Demo video on how to upload your dataset](https://www.youtube.com/watch?v=458soD-Gsv8)\n",
    "- [WorldCereal documentation portal](https://worldcereal.github.io/worldcereal-documentation/rdm/overview.html)\n",
    "- [Free online course on reference data in WorldCereal](https://esa-worldcereal.org/en/resources/free-massive-open-online-courses-mooc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get your private reference data\n",
    "\n",
    "Here we query the [WorldCereal Reference Data Module (RDM)](https://rdm.esa-worldcereal.org/) through the dedicated API to retrieve your private reference data.\n",
    "\n",
    "To learn more about how to interact with the WorldCereal RDM, consult our [dedicated notebook on RDM interaction](https://github.com/WorldCereal/worldcereal-classification/blob/main/notebooks/worldcereal_RDM_demo.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first initiate an interaction session with the RDM:\n",
    "from worldcereal.rdm_api import RdmInteraction\n",
    "rdm = RdmInteraction()\n",
    "\n",
    "# Get a list of your private collections\n",
    "collections = rdm.get_collections(include_public=False, include_private=True)\n",
    "\n",
    "# Extract the collection ID's\n",
    "ids = [col.id for col in collections]\n",
    "print(f'Number of collections found: {len(ids)}')\n",
    "print(ids)\n",
    "\n",
    "# In case you want to look at the metadata of a specific collection, you can use the print_metadata function:\n",
    "collections[0].print_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the list of available collections and their metadata, select your collection of interest. <br>\n",
    "\n",
    "In the next cell, we download the samples contained within your collection. Upon upload to RDM, your dataset has automatically been subsampled taking into account geographical distribution and crop type labels of the observations.<br>\n",
    "\n",
    "You have the option to:\n",
    "- download ALL observations (use `subset = False`)\n",
    "- download only the subsample of your dataset (use `subset = True`)\n",
    "\n",
    "When executing the next cell, you will be prompted to enter the collection ID of your collection of interest. Afterwards, your dataset is downloaded to a `download` folder, located in the folder of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_id = input('Please enter the desired collection ID: ')\n",
    "\n",
    "dwnld_folder = './download'\n",
    "\n",
    "subset = True\n",
    "parquet_file = rdm.download_collection_geoparquet(collection_id, dwnld_folder, subset=subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prepare your reference data\n",
    "\n",
    "Before initiating extractions of EO time series, we run some final preparations/checks on your dataset:\n",
    "\n",
    "- Ensure all required attributes are included in the data\n",
    "\n",
    "- In case you start from a polygon dataset, we convert those polygons to points (centroids).<br>\n",
    "In case the centroid does not intersect with the original polygon, we discard the sample.\n",
    "\n",
    "- We inform you on the total number of samples contained within your dataset, as well as on the geographical spread of the samples. In case high extraction costs are expected, we issue a warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_utils.extractions import prepare_samples_dataframe\n",
    "\n",
    "samples_df = prepare_samples_dataframe(parquet_file, collection_id)\n",
    "samples_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. EO data extractions\n",
    "\n",
    "Now that our GeoDataFrame with reference data is ready, we extract for each reference sample the required EO time series from CDSE using OpenEO.\n",
    "\n",
    "The specific **start and end date** of the time series is automatically set to resp. 9 months prior and 9 months after `valid_time` for each sample.\n",
    "\n",
    "The following **monthly** time series are extracted for the indicated time range:\n",
    "- Sentinel-2 L2A data (all bands)\n",
    "- Sentinel-1 SIGMA0, VH and VV\n",
    "- Average air temperature and precipitation sum derived from AgERA5\n",
    "- Slope and elevation from Copernicus DEM\n",
    "\n",
    "Note that pre-processing of the time series (e.g. cloud masking, temporal compositing) happens on the fly during the extractions.\n",
    "\n",
    "The following cell splits your dataset into several smaller processing jobs and launches these jobs automatically. <br>\n",
    "Depending on the size and spatial spread of your dataset, this step might take a while.\n",
    "\n",
    "Average CDSE credit consumption of one such processing job amounts to 30 credits, but can vary up to 300 credits depending on local data density."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "In case the extraction process might get interrupted during execution, you can just re-run the cell and extractions will resume where they stopped.<br>\n",
    "If you explicitly want to retry any failed processing jobs, you need to set `restart_failed` parameter to `True`.\n",
    "\n",
    "Starting another set of extractions in the same output folder is not possible.\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from worldcereal.extract.common import run_extractions\n",
    "from worldcereal.stac.constants import ExtractionCollection\n",
    "\n",
    "# Define the output folder for the extractions\n",
    "extractions_folder = Path('./extractions')\n",
    "outfolder_col = extractions_folder / collection_id\n",
    "outfolder_col.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save the samples dataframe to a file\n",
    "samples_df_path = outfolder_col / 'samples_gdf.gpkg'\n",
    "samples_df.to_file(samples_df_path, driver='GPKG')\n",
    "\n",
    "run_extractions(\n",
    "    ExtractionCollection.POINT_WORLDCEREAL,\n",
    "    outfolder_col,\n",
    "    samples_df_path,\n",
    "    extract_value=0,\n",
    "    restart_failed=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Inspect results\n",
    "\n",
    "Once the extractions have been completed, we first inspect the job tracking dataframe to find out how many jobs were successfully completed.\n",
    "We also check the success rate of extractions on a sample basis and inspect the cost of the extractions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from worldcereal.extract.common import check_job_status, get_succeeded_job_details\n",
    "\n",
    "status_histogram = check_job_status(outfolder_col)\n",
    "succeeded_jobs = get_succeeded_job_details(outfolder_col)\n",
    "\n",
    "print('************')\n",
    "print('Number of samples successfully extracted:')\n",
    "print(succeeded_jobs['n_samples'].sum())\n",
    "\n",
    "print('************')\n",
    "print('Details of succeeded jobs:')\n",
    "print(succeeded_jobs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we do a more in-depth check of the extracted data by:\n",
    "\n",
    "- manually inspecting a subset of the extracted data (`load_point_extractions` function)\n",
    "\n",
    "- printing statistics for each individual band that was extracted (`get_band_statistics` function)\n",
    "\n",
    "- visualizing time series for randomly selected samples (`visualize_timeseries` function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_utils.extractions import load_point_extractions\n",
    "\n",
    "gdf = load_point_extractions(outfolder_col, subset=True)\n",
    "gdf.head()\n",
    "\n",
    "# keep in mind when inspecting the results: nodata value = 65535"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_utils.extractions import get_band_statistics\n",
    "\n",
    "stats_df = get_band_statistics(outfolder_col)\n",
    "print('************')\n",
    "print('Band statistics:')\n",
    "print(stats_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that by default, the following cell will visualize the NDVI time series for 5 randomly chosen samples.<br>\n",
    "\n",
    "You can change this behaviour by:\n",
    "- specifying the number of samples to visualize (`n_samples` parameter)\n",
    "- specifying the band to visualize (`band` parameter)\n",
    "- specifying a list of sample IDs to visualize (`sample_ids` parameter)\n",
    "\n",
    "Example:\n",
    "`visualize_timeseries(outfolder_col, band=\"S1-SIGMA0-VV\", n_samples=2)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_utils.extractions import visualize_timeseries\n",
    "\n",
    "visualize_timeseries(outfolder_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of Part 1 of this exercise.\n",
    "In the next part, we will use the extracted data to train our custom crop type model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "worldcereal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
